{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1tBmAedYEEJ"
      },
      "source": [
        "## Code requirements: libraries and modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LGrEECeOi_EC",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!python -m spacy download ca_core_news_trf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pi_V5H4g_Df-"
      },
      "outputs": [],
      "source": [
        "import pip\n",
        "from importlib.util import find_spec\n",
        "from scipy.spatial.distance import cosine\n",
        "\n",
        "required_packages = ['transformers', 'torch']\n",
        "\n",
        "for package in required_packages:\n",
        "  if find_spec(package) is None:\n",
        "    print(f'Installing package: {package}...')\n",
        "    pip.main(['install', package])\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForMaskedLM\n",
        "from transformers import AutoTokenizer, pipeline\n",
        "from pprint import pprint\n",
        "from sklearn.decomposition import PCA\n",
        "import numpy as np\n",
        "\n",
        "import spacy\n",
        "lemmatizer = spacy.load('ca_core_news_trf')\n",
        "\n",
        "model_name = 'PlanTL-GOB-ES/roberta-base-ca'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForMaskedLM.from_pretrained(model_name, output_hidden_states = True, )\n",
        "model.eval()\n",
        "model_mask = pipeline('fill-mask', model=model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aBWgbNSHYP1"
      },
      "source": [
        "## Functions to process data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sgEO_TVuC58T"
      },
      "outputs": [],
      "source": [
        "def process_with_BERT(tokenized_text):\n",
        "\n",
        "  # we have to map the tokens to BERT vocabulary indices\n",
        "  indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "  # BERT expects either 1 or 2 sentences. If we give only one sentence,\n",
        "  # we identify its tokens with 1s.\n",
        "  segments_ids = [1] * len(tokenized_text)\n",
        "  # Convert inputs to PyTorch tensors\n",
        "\n",
        "  tokens_tensor = torch.tensor([indexed_tokens])\n",
        "  segments_tensors = torch.tensor([segments_ids])\n",
        "\n",
        "  # Run the text through BERT, and collect all of the hidden states produced\n",
        "  # from all 12 layers.\n",
        "  with torch.no_grad():\n",
        "\n",
        "    outputs = model(tokens_tensor, segments_tensors)\n",
        "\n",
        "    # Evaluating the model will return a different number of objects based on\n",
        "    # how it's  configured in the `from_pretrained` call earlier. In this case,\n",
        "    # becase we set `output_hidden_states = True`, the third item will be the\n",
        "    # hidden states from all layers. See the documentation for more details:\n",
        "    # https://huggingface.co/transformers/model_doc/bert.html#bertmodel\n",
        "    hidden_layers = outputs[1]\n",
        "\n",
        "  return hidden_layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cooEwCpBIs-k"
      },
      "outputs": [],
      "source": [
        "def rearrange_BERT_object(hidden_layers):\n",
        "  token_embeddings = torch.stack(hidden_layers, dim=0)\n",
        "  #print(token_embeddings)\n",
        "  token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
        "  token_embeddings = token_embeddings.permute(1,0,2)\n",
        "  return token_embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "drQbHW_NOnk6"
      },
      "outputs": [],
      "source": [
        "def embeddings_by_sum(hidden_layers):\n",
        "  # we rearrange the hidden layers so that we can more easily iterate over tokens\n",
        "  token_embeddings = rearrange_BERT_object(hidden_layers)\n",
        "  token_vecs_sum = []\n",
        "\n",
        "  for token in token_embeddings:\n",
        "    # 'token' is a [12 x 768] tensor\n",
        "    # Sum the vectors from the last four layers.\n",
        "    sum_vec = torch.sum(token[-4:], dim=0)\n",
        "    # Use 'sum_vec' to represent token.\n",
        "    token_vecs_sum.append(sum_vec)\n",
        "\n",
        "  return token_vecs_sum"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMe70WEeqBHf"
      },
      "source": [
        "## Computing word point in sentence and parsing data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MxfUW-8fBorp"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "pd.set_option('display.float_format', lambda x: '%.9f' % x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6MwUb1qir9qF"
      },
      "outputs": [],
      "source": [
        "def get_sentences_from_csv_file(file):\n",
        "  df = pd.read_csv(file, header=None, names=['Sense_1', 'Sense_2'])\n",
        "\n",
        "  sentences_sense_1 = []\n",
        "  sentences_sense_2 = []\n",
        "\n",
        "  for i in range(0,len(df['Sense_1'])):\n",
        "    sentences_sense_1.append(\"[CLS] \" + df['Sense_1'][i] + \" [SEP]\")\n",
        "\n",
        "  for i in range(0,len(df['Sense_2'])):\n",
        "    sentences_sense_2.append(\"[CLS] \" + df['Sense_2'][i] + \" [SEP]\")\n",
        "\n",
        "  return (sentences_sense_1, sentences_sense_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D01S-NZhfH9e"
      },
      "outputs": [],
      "source": [
        "def tokenize_sentences(sentences):\n",
        "  tokenized_sentences = []\n",
        "  for sentence in sentences:\n",
        "    tokenized_sentences.append(tokenizer.tokenize(sentence))\n",
        "  return tokenized_sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jEFphUH5INpK"
      },
      "outputs": [],
      "source": [
        "def merged_bpe (llista):\n",
        "  for unit in llista:\n",
        "    merged_text = []\n",
        "    i = 0\n",
        "    while i < len(llista):\n",
        "      if i == 0 or llista[i].startswith('Ġ'):\n",
        "          merged_text.append(llista[i][1:])\n",
        "          i += 1\n",
        "      else:\n",
        "          merged_text[-1] += llista[i]  # Append the subword without the '##' prefix\n",
        "          i += 1\n",
        "\n",
        "  return merged_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pzfFFoy1yIFJ"
      },
      "outputs": [],
      "source": [
        "def find_word_in_bpe(llista, word):\n",
        "  adapted_word = adapt_word_to_tokenizer(word).lower()\n",
        "  current_word = \"\"\n",
        "  indices = []\n",
        "  i = 0\n",
        "  while i < len(llista):\n",
        "    if (llista[i].startswith('-')):\n",
        "      i += 1\n",
        "      while not llista[i].startswith('Ġ'):\n",
        "        i += 1\n",
        "      continue\n",
        "    if i == 0 or llista[i].startswith('Ġ'):\n",
        "      filtered_current_word = ''.join(filter(str.isalpha, current_word.lower()))\n",
        "      if filtered_current_word == adapted_word:\n",
        "        return indices\n",
        "      current_word = llista[i][1:]\n",
        "      indices.clear()\n",
        "    else:\n",
        "      current_word += llista[i]\n",
        "    indices.append(i)\n",
        "    i += 1\n",
        "  if current_word.lower() == adapted_word:\n",
        "    return indices\n",
        "  return []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ojxymMzx9iT"
      },
      "outputs": [],
      "source": [
        "def adapt_word_to_tokenizer(word):\n",
        "  word_converter_dictionary={'à': 'Ãł','è': 'Ã¨', 'ò': 'Ã²', 'é': 'Ã©', 'í': 'ÃŃ', 'ó': 'Ã³', 'ú': 'Ãº'} #dièresi, ç, l·l\n",
        "  adapted_string = ''\n",
        "  for character in word:\n",
        "    if character in word_converter_dictionary:\n",
        "      adapted_string += word_converter_dictionary[character]\n",
        "    else:\n",
        "      adapted_string += character\n",
        "  return adapted_string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sTJ1CSOk6srI"
      },
      "outputs": [],
      "source": [
        "def find_target_word_lemmatizing(lemmatized_words, sentence):\n",
        "  lemmatized_sentence = lemmatizer(sentence)\n",
        "  for index, word in enumerate(lemmatized_sentence):\n",
        "    if (word.lemma_.lower() in lemmatized_words):\n",
        "      return str(word)\n",
        "  return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MnRjEAhJz-sh"
      },
      "outputs": [],
      "source": [
        "def find_word_indices(tokenized_sentences, sentences, words):\n",
        "  indices = []\n",
        "  #we need to adapt the word because the tokenizer does not take accents correctly\n",
        "  adapted_word = adapt_word_to_tokenizer(words[0]).lower()\n",
        "  for idx, sent in enumerate(tokenized_sentences):\n",
        "    target_word = find_target_word_lemmatizing(words, sentences[idx])\n",
        "    if (target_word == None):\n",
        "      print(f\"{words[0]}, {idx} - {sentences[idx]}\")\n",
        "      indices.append(None)\n",
        "      continue\n",
        "    word_indices_in_sent = find_word_in_bpe(sent, target_word)\n",
        "\n",
        "    #just to check errors\n",
        "    if len(word_indices_in_sent) == 0:\n",
        "      print(f\"Word: [{words[0]} - {idx}, (adapted as: {adapted_word}), lemmatized as: {target_word}] not found in sentence: [{sent}]\")\n",
        "      indices.append(None)\n",
        "      continue\n",
        "\n",
        "    indices.append(word_indices_in_sent)\n",
        "\n",
        "  return indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m-cuE92hhaLG"
      },
      "outputs": [],
      "source": [
        "def compute_word_position_in_sentences(tokenized_sentences, sentences, words):\n",
        "  indices_in_sents = find_word_indices(tokenized_sentences, sentences, words)\n",
        "\n",
        "  vecs_sums = []\n",
        "\n",
        "  for i in range(0, len(tokenized_sentences)):\n",
        "    if indices_in_sents[i] == None:\n",
        "      continue\n",
        "    hidden_layers = process_with_BERT(tokenized_sentences[i])\n",
        "    token_vecs_sum = embeddings_by_sum(hidden_layers)\n",
        "    pos_sum = token_vecs_sum[indices_in_sents[i][0]]\n",
        "    if len(indices_in_sents[i]) > 1:\n",
        "      for idx in range(1,len(indices_in_sents[i])):\n",
        "        pos_sum += token_vecs_sum[indices_in_sents[i][idx]]\n",
        "    vecs_sums.append(pos_sum/len(indices_in_sents[i]))\n",
        "\n",
        "  return vecs_sums\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUaudKedqi8T"
      },
      "source": [
        "## Analyze word in sentence"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def cosine_similarity(point1, point2):\n",
        "  return 1 - cosine(point1, point2)"
      ],
      "metadata": {
        "id": "u8v8NAk_HR_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_stabilization(word, context, positions, stabilization_df):\n",
        "  if len(positions) == 0:\n",
        "    return\n",
        "  #initializing variables to compare with previous result\n",
        "  previous_accumulated = positions[0].clone()\n",
        "  accumulated = positions[0].clone()\n",
        "  previous_similarity = None\n",
        "\n",
        "  for i in range(1,len(positions)):\n",
        "    accumulated += positions[i]\n",
        "    #for each 10 results\n",
        "    if i%10 == 0:\n",
        "      #compute cosine similarity, compute similarity difference with the last similarity\n",
        "      similarity = cosine_similarity(previous_accumulated, accumulated)\n",
        "      similarity_difference = abs(similarity - previous_similarity) if previous_similarity != None else 0\n",
        "      #append it to the dataframe\n",
        "      stabilization_df.loc[len(stabilization_df.index)] = [word,i, similarity, similarity_difference, context]\n",
        "      #update the variables to compare\n",
        "      previous_accumulated = accumulated.clone()\n",
        "      previous_similarity = similarity"
      ],
      "metadata": {
        "id": "N-Z4pdnFGSDn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l-KVes43dPdW"
      },
      "outputs": [],
      "source": [
        "def analyse_word_in_sentence(sentences_sense_1, sentences_sense_2, words, stabilization_df):\n",
        "  tokenized_sents_sense_1 = tokenize_sentences(sentences_sense_1)\n",
        "  tokenized_sents_sense_2 = tokenize_sentences(sentences_sense_2)\n",
        "\n",
        "  word_positions_in_sents_sense_1 = compute_word_position_in_sentences(tokenized_sents_sense_1, sentences_sense_1, words)\n",
        "  word_positions_in_sents_sense_2 = compute_word_position_in_sentences(tokenized_sents_sense_2, sentences_sense_2, words)\n",
        "\n",
        "  analyze_stabilization(words[0], \"neo\", word_positions_in_sents_sense_1, stabilization_df)\n",
        "  analyze_stabilization(words[0], \"no_neo\", word_positions_in_sents_sense_2, stabilization_df)\n",
        "\n",
        "  #we only care about comparing words with the new sense with the ones in the old sense\n",
        "  #because of this, we are going to compare old senses with themselves and new senses with old senses\n",
        "  avg_point_sense_1 = sum(word_positions_in_sents_sense_1)/len(word_positions_in_sents_sense_1)\n",
        "  avg_point_sense_2 = sum(word_positions_in_sents_sense_2)/len(word_positions_in_sents_sense_2)\n",
        "  cosine_avg = cosine_similarity(avg_point_sense_1, avg_point_sense_2)\n",
        "  print(cosine_avg)\n",
        "  return cosine_avg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L3DJcqAnsY4J"
      },
      "outputs": [],
      "source": [
        "def analyse_word_in_sentence_from_csv(file, words, stabilization_df):\n",
        "  (sentences_sense_1, sentences_sense_2) = get_sentences_from_csv_file(file)\n",
        "  return analyse_word_in_sentence(sentences_sense_1, sentences_sense_2, words, stabilization_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8LuEgfiPe9fL"
      },
      "outputs": [],
      "source": [
        "def analyze_multiple_words_in_sentences_from_csv(files_and_words, stabilization_df):\n",
        "  df = pd.DataFrame({'word': [], 'cosine': []})\n",
        "  for (file, words) in files_and_words:\n",
        "    #(new_sense, old_sense, comparison_new_old_sense, difference) =\n",
        "    cosine_avg = analyse_word_in_sentence_from_csv(file, words, stabilization_df)\n",
        "    df.loc[len(df.index)] = [words[0], cosine_avg]\n",
        "  return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyTibc3tDjkD"
      },
      "source": [
        "# Analyze the selected verbs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EiAhxy9qN0VU"
      },
      "outputs": [],
      "source": [
        "# useful to debug the lemmatizer\n",
        "sentence_lemmatize = \"això, perduda en un munt de paraules sense sentit que em condueixen a un bogeria sense fi, una bogeria dolça, complaent, anestesiant . Una bogeria que m'allibera de tot això tan imperfecte que hi ha al meu voltant, una bogeria que trobo més real que tot\"  #@param {type:\"string\"}\n",
        "lemmatized_sentence = lemmatizer(sentence_lemmatize)\n",
        "for token in lemmatized_sentence:\n",
        "  print(token.lemma_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3T3BV7Bwgsw"
      },
      "source": [
        "Useful to compute stabilization and save the stabilization data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stabilization_df = pd.DataFrame({'paraula': [], 'num_contextos': [], 'cosine': [], 'diferencia': [], 'context': []})"
      ],
      "metadata": {
        "id": "7MDZENo9I6o1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stabilization_df.to_csv('stabilization_data.csv', float_format=lambda x: '%.9f' % x)"
      ],
      "metadata": {
        "id": "hbvBvxxGP_TP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stabilization_df"
      ],
      "metadata": {
        "id": "NRRv9MWCMBkH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> Neologic verbs\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EcvPM615_Kr3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oD9sVWeXg4_O"
      },
      "outputs": [],
      "source": [
        "verbs_analitzats = analyze_multiple_words_in_sentences_from_csv([('anestesiar.csv', ['anestesiar', 'anestesiat', 'anestesiant'])], stabilization_df)\n",
        "print(verbs_analitzats)\n",
        "\n",
        "#we save it to a file so that if we want to use the compute data, we don't have to recompute it again\n",
        "#verbs_analitzats.to_csv('verbs.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Doi23JTowfRF"
      },
      "outputs": [],
      "source": [
        "verbs_analitzats = analyze_multiple_words_in_sentences_from_csv([('maquillar.csv', ['maquillar', 'maquillat'])], stabilization_df)\n",
        "print(verbs_analitzats)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EJmzUlGzy-Lm"
      },
      "outputs": [],
      "source": [
        "verbs_analitzats = analyze_multiple_words_in_sentences_from_csv([('blindar.csv', ['blindar', 'blindat', 'blindir'])], stabilization_df)\n",
        "print(verbs_analitzats)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WTMXGD4vz7BA"
      },
      "outputs": [],
      "source": [
        "verbs_analitzats = analyze_multiple_words_in_sentences_from_csv([('arrasar.csv', ['arrasar', 'arrasat'])], stabilization_df)\n",
        "print(verbs_analitzats)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ghu1uHwtJrFC"
      },
      "outputs": [],
      "source": [
        "verbs_analitzats = analyze_multiple_words_in_sentences_from_csv([('disparar.csv', ['disparar', 'disparat'])], stabilization_df)\n",
        "print(verbs_analitzats)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GmqKZyfPLFrV"
      },
      "outputs": [],
      "source": [
        "verbs_analitzats = analyze_multiple_words_in_sentences_from_csv([('quallar.csv', ['quallar', 'quallat', 'quallir'])], stabilization_df)\n",
        "print(verbs_analitzats)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ggnaoNQGLhcR"
      },
      "outputs": [],
      "source": [
        "verbs_analitzats = analyze_multiple_words_in_sentences_from_csv([('sacsejar.csv', ['sacsejar', 'sacsegen', 'sacsegar', 'sacsejat'])], stabilization_df)\n",
        "print(verbs_analitzats)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WE8HKpOgwiiy"
      },
      "source": [
        "> Monosemic verbs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1o6kE6u5tzN3"
      },
      "outputs": [],
      "source": [
        "verbs_analitzats = analyze_multiple_words_in_sentences_from_csv([('nevar.csv', ['nevar'])])\n",
        "print(verbs_analitzats)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J2iRHMUmvF9M"
      },
      "outputs": [],
      "source": [
        "verbs_analitzats = analyze_multiple_words_in_sentences_from_csv([('agilitzar.csv', ['agilitzar'])])\n",
        "print(verbs_analitzats)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OgkxbLwj2wK-"
      },
      "outputs": [],
      "source": [
        "verbs_analitzats = analyze_multiple_words_in_sentences_from_csv([('agredir.csv', ['agredir'])])\n",
        "print(verbs_analitzats)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YlbvVkmG3xO6"
      },
      "outputs": [],
      "source": [
        "verbs_analitzats = analyze_multiple_words_in_sentences_from_csv([('conversar.csv', ['conversar'])])\n",
        "print(verbs_analitzats)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NHCRnlGN5ymn"
      },
      "outputs": [],
      "source": [
        "verbs_analitzats = analyze_multiple_words_in_sentences_from_csv([('esmentar.csv', ['esmentar'])])\n",
        "print(verbs_analitzats)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-JX_3T4xwUpL"
      },
      "outputs": [],
      "source": [
        "verbs_analitzats = analyze_multiple_words_in_sentences_from_csv([('esmenar.csv', ['esmenar'])])\n",
        "print(verbs_analitzats)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qqse-uCa7B1B"
      },
      "outputs": [],
      "source": [
        "verbs_analitzats = analyze_multiple_words_in_sentences_from_csv([('fullejar.csv', ['fullejar'])])\n",
        "print(verbs_analitzats)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_BhffzgZAdzN"
      },
      "outputs": [],
      "source": [
        "verbs_analitzats = analyze_multiple_words_in_sentences_from_csv([('inculcar.csv', ['inculcar'])])\n",
        "print(verbs_analitzats)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJGaHAk1Bi7c"
      },
      "outputs": [],
      "source": [
        "verbs_analitzats = analyze_multiple_words_in_sentences_from_csv([('numerar.csv', ['numerar', 'numerat'])])\n",
        "print(verbs_analitzats)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t-e0Z2r3Bp4m"
      },
      "outputs": [],
      "source": [
        "verbs_analitzats = analyze_multiple_words_in_sentences_from_csv([('precisar.csv', ['precisar'])])\n",
        "print(verbs_analitzats)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1awucQKaDHYX"
      },
      "outputs": [],
      "source": [
        "verbs_analitzats = analyze_multiple_words_in_sentences_from_csv([('teclejar.csv', ['teclejar'])])\n",
        "print(verbs_analitzats)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
